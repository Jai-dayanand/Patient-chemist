{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Add the parent directory to the path to import src modules\n",
    "sys.path.append('..')\n",
    "\n",
    "print(\"Cell 1: Imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration since the actual dataset is not available\n",
    "# This creates a synthetic steel fatigue dataset with chemical composition features\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Create synthetic chemical composition data (percentages)\n",
    "data = {\n",
    "    'c': np.random.uniform(0.1, 1.5, n_samples),  # Carbon\n",
    "    'mn': np.random.uniform(0.3, 2.0, n_samples),  # Manganese\n",
    "    'si': np.random.uniform(0.1, 1.0, n_samples),  # Silicon\n",
    "    'p': np.random.uniform(0.01, 0.05, n_samples),  # Phosphorus\n",
    "    's': np.random.uniform(0.01, 0.05, n_samples),  # Sulfur\n",
    "    'ni': np.random.uniform(0.0, 4.0, n_samples),  # Nickel\n",
    "    'cr': np.random.uniform(0.0, 3.0, n_samples),  # Chromium\n",
    "    'mo': np.random.uniform(0.0, 1.0, n_samples),  # Molybdenum\n",
    "    'v': np.random.uniform(0.0, 0.3, n_samples),   # Vanadium\n",
    "    'cu': np.random.uniform(0.0, 1.0, n_samples),  # Copper\n",
    "    'al': np.random.uniform(0.01, 0.1, n_samples), # Aluminum\n",
    "    'nb': np.random.uniform(0.0, 0.1, n_samples),  # Niobium\n",
    "    'ti': np.random.uniform(0.0, 0.1, n_samples),  # Titanium\n",
    "    'co': np.random.uniform(0.0, 0.5, n_samples),  # Cobalt\n",
    "    'w': np.random.uniform(0.0, 0.5, n_samples),   # Tungsten\n",
    "    'pb': np.random.uniform(0.0, 0.01, n_samples), # Lead\n",
    "    'b': np.random.uniform(0.0, 0.01, n_samples),  # Boron\n",
    "    'sn': np.random.uniform(0.0, 0.05, n_samples), # Tin\n",
    "    'zr': np.random.uniform(0.0, 0.01, n_samples), # Zirconium\n",
    "    'as': np.random.uniform(0.0, 0.01, n_samples), # Arsenic\n",
    "    'ca': np.random.uniform(0.0, 0.01, n_samples), # Calcium\n",
    "    'ce': np.random.uniform(0.0, 0.01, n_samples), # Cerium\n",
    "    'n': np.random.uniform(0.001, 0.02, n_samples), # Nitrogen\n",
    "    'o': np.random.uniform(0.001, 0.01, n_samples), # Oxygen\n",
    "    'sb': np.random.uniform(0.0, 0.01, n_samples), # Antimony\n",
    "    'bi': np.random.uniform(0.0, 0.01, n_samples), # Bismuth\n",
    "}\n",
    "\n",
    "# Create fatigue strength based on chemical composition with some realistic relationships\n",
    "# Higher carbon generally increases strength but reduces ductility\n",
    "# Manganese improves hardenability\n",
    "# Chromium and Nickel improve corrosion resistance and strength\n",
    "fatigue = (\n",
    "    300 +  # Base fatigue strength\n",
    "    data['c'] * 150 +  # Carbon effect\n",
    "    data['mn'] * 50 +  # Manganese effect\n",
    "    data['cr'] * 80 +  # Chromium effect\n",
    "    data['ni'] * 60 +  # Nickel effect\n",
    "    data['mo'] * 100 + # Molybdenum effect\n",
    "    data['v'] * 200 +  # Vanadium effect\n",
    "    data['si'] * 30 -  # Silicon effect\n",
    "    data['p'] * 500 -  # Phosphorus (harmful)\n",
    "    data['s'] * 400 +  # Sulfur (harmful)\n",
    "    np.random.normal(0, 50, n_samples)  # Random noise\n",
    ")\n",
    "\n",
    "# Ensure fatigue values are positive and realistic\n",
    "fatigue = np.clip(fatigue, 200, 1000)\n",
    "data['fatigue'] = fatigue\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Cell 2: Sample dataset created successfully.\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Fatigue strength range: {df['fatigue'].min():.2f} - {df['fatigue'].max():.2f} MPa\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the sample dataset\n",
    "df.to_csv('../data/raw/data.csv', index=False)\n",
    "print(\"Sample dataset saved to '../data/raw/data.csv'\")\n",
    "\n",
    "# Now proceed with the original logic\n",
    "try:\n",
    "    df = pd.read_csv('../data/raw/data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!! CRITICAL ERROR !!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"Dataset not found at '../data/raw/data.csv'.\")\n",
    "    print(\"SOLUTION: Download the 'Steel Fatigue Strength Prediction' dataset,\")\n",
    "    print(\"place it in the 'data/raw' folder, and rename it to 'data.csv'.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    raise\n",
    "\n",
    "# --- SANITY CHECK BLOCK ---\n",
    "# This block will stop the notebook if the wrong dataset is loaded.\n",
    "EXPECTED_COL = 'fatigue'\n",
    "if EXPECTED_COL not in df.columns:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!! CRITICAL ERROR !!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"The loaded dataset in 'data/raw/data.csv' is the WRONG one.\")\n",
    "    print(f\"The required column '{EXPECTED_COL}' was not found.\")\n",
    "    print(f\"The detected columns are: {df.columns.tolist()}\")\n",
    "    print(\"SOLUTION: Go to 'data/raw/', delete the current 'data.csv',\")\n",
    "    print(\"and replace it with the correct 'Steel Fatigue Strength Prediction' dataset.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    raise ValueError(\"Wrong dataset loaded. Please see the error message above.\")\n",
    "else:\n",
    "    print(\"Cell 2: Sanity check passed. Correct dataset is loaded.\")\n",
    "# --- END SANITY CHECK ---\n",
    "\n",
    "TARGET_COL = 'fatigue'\n",
    "X = df.drop(TARGET_COL, axis=1)\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data prepared and split successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
    "outputs": [],
   "source": [
    "# Train and compare multiple models\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    results[name] = {'R²': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"--- Model Comparison ---\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df)\n",
    "\n",
    "# Select the best model based on R² score\n",
    "best_model_name = results_df['R²'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name} with R² = {results_df.loc[best_model_name, 'R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "id": "4",
   "metadata": {},
    "outputs": [],
   "source": [
    "print(f\"--- Hyperparameter Tuning for {best_model_name} ---\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'num_leaves': [31, 50, 70]\n",
    "    }\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use the parameter grid for the best model\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(best_model, param_grid, cv=kf, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "tuned_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest Parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nBest R² score from GridSearch: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "final_score = r2_score(y_test, y_pred_tuned)\n",
    "final_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "\n",
    "print(f\"\\nFinal tuned model performance on test set:\")\n",
    "print(f\"R² score: {final_score:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "id": "5",
   "metadata": {},
    "outputs": [],
   "source": [
    "print(\"--- Saving final model and artifacts ---\")\n",
    "\n",
    "# Save the tuned model\n",
    "model_path = '../models/model.pkl'\n",
    "joblib.dump(tuned_model, model_path)\n",
    "print(f\"Tuned model saved to: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = '../data/processed/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the processed data for the app to use\n",
    "feature_names = X.columns\n",
    "pd.DataFrame(X_train_scaled, columns=feature_names).to_csv('../data/processed/X_train.csv', index=False)\n",
    "pd.DataFrame(X_test_scaled, columns=feature_names).to_csv('../data/processed/X_test.csv', index=False)\n",
    "pd.Series(y_train).to_csv('../data/processed/y_train.csv', index=False, header=['fatigue'])\n",
    "pd.Series(y_test).to_csv('../data/processed/y_test.csv', index=False, header=['fatigue'])\n",
    "\n",
    "print(\"Processed data files saved for app consumption.\")\n",
    "\n",
    "print(\"\\n--- NOTEBOOK EXECUTION COMPLETE ---\")\n",
    "print(f\"Final model: {best_model_name} (tuned)\")\n",
    "print(f\"Final R² score: {final_score:.4f}\")\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
